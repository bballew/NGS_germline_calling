#!/usr/bin/env python3
# Authors: Shalabh Suman, Bari Ballew


'''Joint variant calling with GATK HaplotypeCaller and Google DeepVariant.

Notes:
    This pipeline is for use with the production germline pipeline.
    Therefore, some things are assumed, e.g. that input bams are indexed,
    that the reference genome is appropriately indexed, etc.  The pipeline
    will halt if these assumptions are not true, but there are no rules
    to perform these tasks.

Input:
    Customized config.yaml, sorted/indexed bams

Output:
    Merged multi-sample VCFs, one called with HaplotypeCaller, and one
    called with DeepVariant

'''


import os
import glob

# reference the config file
conf = os.environ.get("conf")
configfile: conf

# import variables from the config file
refGenome = config['refGenome']
gatkPath = config['gatkPath'].rstrip('/') + '/'  # ensure one trailing slash
intervalFile = config['intervalFile']
bedFile = config['bedFile']
outDir = config['outputDir'].rstrip('/') + '/'
inDir = config['inputDir'].rstrip('/') + '/'
tempDir=config['tempDir'].rstrip('/') + '/'
logDir = config['logDir'].rstrip('/') + '/'
modelPath = config['modelPath']
useShards = config['useShards']
if useShards:
    numShards = config['numShards']
    maxShards = "{:05d}".format(numShards)
    s = list(range(numShards))
    shardsList = ["%05d" % n for n in s]

# derive additional refgenome variables
refFile = os.path.basename(refGenome)
refDir = os.path.dirname(refGenome)
refNoExt = os.path.splitext(refFile)[0]
dictionaryFile = refDir + '/' + refNoExt + '.dict'

# set a datetime stamp for GATK DBImport step
dt = str(config['dt'])

# read all *.bam filenames in input directory into list (assume bams are indexed)
bamList = [f for f in os.listdir(inDir) if f.endswith('.bam')]

# generate sampleList based on bam filenames
sampleList = [os.path.splitext(f)[0] for f in bamList]

# read in chromosome list from reference dict file (assumes the dict is already created)
chromList = []
with open(dictionaryFile) as f:
    next(f)
    for line in f:
        f1 = line.split("\t")[1]
        f2 = f1.split(":")[1]
        if "M" not in f2:  # exclude chrM, otherwise creates an empty interval file which GATK doesn't like
            chromList.append(f2)

def get_DBImport_path1(wildcards):
    return(glob.glob(outDir + 'HaplotypeCaller/DBImport/' + dt + '_' + wildcards.chrom + '/' + wildcards.chrom + '*/genomicsdb_meta_dir/genomicsdb_meta*.json'))

def get_DBImport_path2(wildcards):
    path = ''.join(glob.glob(outDir + 'HaplotypeCaller/DBImport/' + dt + '_' + wildcards.chrom + '/*/__*/'))
    myList = []
    if os.path.exists(path):
        myList = ['AD.tdb', 'AD_var.tdb', 'ALT.tdb', 'ALT_var.tdb', 'BaseQRankSum.tdb', '__book_keeping.tdb.gz', '__coords.tdb', 'DP_FORMAT.tdb', 'DP.tdb', 'DS.tdb', 'END.tdb', 'ExcessHet.tdb', 'FILTER.tdb', 'FILTER_var.tdb', 'GQ.tdb', 'GT.tdb', 'GT_var.tdb', 'ID.tdb', 'ID_var.tdb', 'InbreedingCoeff.tdb', 'MIN_DP.tdb', 'MLEAC.tdb', 'MLEAC_var.tdb', 'MLEAF.tdb', 'MLEAF_var.tdb', 'MQRankSum.tdb', 'PGT.tdb', 'PGT_var.tdb', 'PID.tdb', 'PID_var.tdb', 'PL.tdb', 'PL_var.tdb', 'QUAL.tdb', 'RAW_MQandDP.tdb', 'ReadPosRankSum.tdb', 'REF.tdb', 'REF_var.tdb', 'SB.tdb', '__tiledb_fragment.tdb']
        myList = [path + file for file in myList]
    return(myList)

report: 'report/workflow.rst'


rule all:
    input:
        outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz.tbi',
        outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz.tbi',
        expand(outDir + 'deepVariant/called/vcfs/{sample}_all_chroms.vcf.gz', sample=sampleList),
        outDir + 'deepVariant/genotyped/build_final.vcf.gz.tbi'

#######################################################################################################################
################################################ SPLIT REGIONS BY CHROM ###############################################
#######################################################################################################################

rule split_bed_file:
    '''Separates bed regions by chromosome.

    For DV:
    If you're not assigning a number of shards by which to divide
    and parallelize, then the pipeline will parallelize by chrom.
    To do this, we take the bed file (e.g. exome capture region)
    and split the regions by chromosome.  Subsequent steps are run
    concurrently on each of the single-chromosome bed files.

    For GATK:
    HaplotypeCaller can't be parallelized per task (e.g. threads),
    so must be run over sub-regions if you want parallelization.
    **Do we want to use the old 4000-region bed file, or is by-chrom
    sufficient?

    Note that grep exits with 0 if a match is found, 1 if no match,
    and 2 if error.  Snakemake looks for exit codes of 0 to determine
    that a job finished successfully.  No match is an acceptable outcome
    here, so the shell command below should allow match or no match.

    don't love the ||true solution; what will it do for exit > 1?
    '''
    input:
        bed = bedFile,
        interval = intervalFile
    output:
        bed = outDir + 'split_regions/{chrom}.bed',
        interval = outDir + 'split_regions/{chrom}.intervals'
    benchmark:
        'run_times/split_bed_file/{chrom}.tsv'
    shell:
        'grep "^{wildcards.chrom}[[:space:]]" {input.bed} > {output.bed} || true;'
        'grep "^{wildcards.chrom}:" {input.interval} > {output.interval} || true'
        #'grep -m1 {wildcards.chrom} {input}; if [ $? -lt 2 ]; then grep {wildcards.chrom} {input} > {output}; fi'

#######################################################################################################################
################################################ GATK HAPLOTYPE CALLER ################################################
#######################################################################################################################

# rule HC_call_variants:
#     '''Call gVCFs with GATK4

#     I've commented out all the threads sections of the rules.  For
#     GATK4 specifically, it doesn't offer multithreading except with 
#     the spark implementations of its tools, which is still in beta.
#     Snakemake cannot re-architect software to become parallel.

#     If we want to run this rule in parallel, we will need to split
#     regions as we did previously.

#     Note that -new-qual is deprecated as of 4.1.0.0.
#     '''
#     input:
#         ref = refGenome,
#         i1 = refGenome + '.amb',
#         i2 = refGenome + '.ann',
#         i3 = refGenome + '.bwt',
#         i4 = refGenome + '.pac',
#         i5 = refGenome + '.sa',
#         i6 = refGenome + '.fai',
#         i7 = dictionaryFile,
#         interval =  intervalFile,
#         bam = inDir + '{sample}.bam',
#         bai = inDir + '{sample}.bam.bai'
#     output:
#         gvcf = outDir + 'HaplotypeCaller/called/{sample}.g.vcf'
#     params:
#         e = gatkPath
#     # threads: nt
#     # singularity: 'docker://broadinstitute/gatk'
#     shell:
#         '{params.e}gatk --java-options "-Xmx4G" HaplotypeCaller \
#             -R {input.ref} \
#             -I {input.bam} \
#             -ERC GVCF \
#             -L {input.interval} \
#             -O {output.gvcf} \
#             -new-qual \
#             -G StandardAnnotation \
#             -G StandardHCAnnotation'

rule HC_call_variants:
    '''Call gVCFs with GATK4
    Runs over each chrom in parallel.
    '''
    input:
        ref = refGenome,
        i1 = refGenome + '.amb',
        i2 = refGenome + '.ann',
        i3 = refGenome + '.bwt',
        i4 = refGenome + '.pac',
        i5 = refGenome + '.sa',
        i6 = refGenome + '.fai',
        i7 = dictionaryFile,
        interval =  outDir + 'split_regions/{chrom}.intervals',
        bam = inDir + '{sample}.bam',
        bai = inDir + '{sample}.bam.bai'
    output:
        gvcf = temp(outDir + 'HaplotypeCaller/called/{chrom}/{sample}.g.vcf'),
        idx = temp(outDir + 'HaplotypeCaller/called/{chrom}/{sample}.g.vcf.idx')
    params:
        e = gatkPath
    benchmark:
        'run_times/HC_call_variants/{chrom}_{sample}.tsv'
    shell:
        '{params.e}gatk --java-options "-Xmx4G" HaplotypeCaller \
            -R {input.ref} \
            -I {input.bam} \
            -ERC GVCF \
            -L {input.interval} \
            -O {output.gvcf} \
            -new-qual \
            -G StandardAnnotation \
            -G StandardHCAnnotation'

rule HC_compress_gvcfs:
    '''Zip and index gVCFs
    '''
    input:
        gvcf = outDir + 'HaplotypeCaller/called/{chrom}/{sample}.g.vcf',
        idx = outDir + 'HaplotypeCaller/called/{chrom}/{sample}.g.vcf.idx'
    output:
        temp(outDir + 'HaplotypeCaller/called/{chrom}/{sample}.g.vcf.gz'),
        temp(outDir + 'HaplotypeCaller/called/{chrom}/{sample}.g.vcf.gz.tbi')
    benchmark:
        'run_times/HC_compress_gvcfs/{chrom}_{sample}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix bgzip;'
        'bgzip {input.gvcf}; tabix -p vcf {input.gvcf}.gz'

rule HC_concat_gvcfs:
    '''
    Not clear whether it would be fastest to concat per-chrom gvcfs and
    then genotype, or genotype and then concat.
    '''
    input:
        vcfList = expand(outDir + 'HaplotypeCaller/called/{chrom}/{{sample}}.g.vcf.gz', chrom=chromList),
        indexList = expand(outDir + 'HaplotypeCaller/called/{chrom}/{{sample}}.g.vcf.gz.tbi', chrom=chromList)
    output:
        gz = outDir + 'HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz'
    params:
        e = gatkPath,
        gvcfList = lambda wildcards, input:" -I ".join(input.vcfList)
    benchmark:
        'run_times/HC_concat_gvcfs/{sample}.tsv'
    shell:
        '{params.e}gatk --java-options "-Xmx4G" GatherVcfs -I {params.gvcfList} -O {output.gz}'

rule HC_index_gvcf:
    input:
        outDir + 'HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz'
    output:
        tbi = outDir + 'HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz.tbi'
    benchmark:
        'run_times/HC_index_gvcf/{sample}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input}'

rule HC_consolidate_gvcfs:
    '''Split samples by chromosome

    The output of this step includes some files that are in subdirectories
    with unpredictable names.  I've attempted to include them in the input
    of the next rule via glob in the functions below (o5 and o6 in rule
    HC_genotypeGVCFs) but this is still being tested.

    Note that DBImport requires a new or empty directory for 
    --genomicsdb-workspace-path.  Possible issue when resuming a pipeline.
    Snakemake's implicit directory management results in an error when
    DBImport finds that the workspace path already exists (even though it
    seems empty?).  Removing the directory just prior to running DBImport
    seems to solve this problem, but will be problematic on resuming the 
    pipeline.  Added a datetime stamp (dt) to the dir to help address this.
    However, this datetime stamp will need to be overriden if the pipeline
    is stopped and then resumed prior to completion of rule HC_genotypeGVCFs.

    What exactly is GenomicsDB for?  GenotypeGVCFs can only take one single
    input.  GenomicsDB is a method of consolidating gvcfs across samples,
    to provide the input for genotyping.  The alternative is to use 
    CombineGVCFs.  See 
    https://software.broadinstitute.org/gatk/documentation/article?id=11813 
    for more details.
    '''
    input: 
        gvcfList = expand(outDir + "HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz", sample=sampleList),
        indexList = expand(outDir + "HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz.tbi", sample=sampleList)
    output:
        o1 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vcfheader.vcf',
        o2 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vidmap.json',
        o3 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/callset.json',
        o4 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/__tiledb_workspace.tdb'
    params:
        e = gatkPath,
        gvcfList = lambda wildcards, input:" -V ".join(input.gvcfList),
        interval = '{chrom}',
        db = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}'
    benchmark:
        'run_times/HC_consolidate_gvcfs/{chrom}.tsv'
    shell:
        'rm -r {params.db}; {params.e}gatk --java-options "-Xmx4G" GenomicsDBImport -V {params.gvcfList} --genomicsdb-workspace-path {params.db} -L {params.interval}'

rule HC_genotype_gvcfs:
    '''Joint genotyping
    '''
    input:
        ref = refGenome,
        interval =  intervalFile,
        o1 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vcfheader.vcf',
        o2 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vidmap.json',
        o3 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/callset.json',
        o4 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/__tiledb_workspace.tdb',
        o5 = get_DBImport_path1,
        o6 = get_DBImport_path2
    output:
        vcf = outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz',
        idx = outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz.tbi'
    params:
        e = gatkPath,
        db = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}',
        t = tempDir + 'HC_genotype_gvcfs/{chrom}/'
    benchmark:
        'run_times/HC_genotype_gvcfs/{chrom}.tsv'
    shell:
        'mkdir -p {params.t}; \
        {params.e}gatk --java-options "-Xmx4G" GenotypeGVCFs \
            -R {input.ref} \
            -V gendb://{params.db} \
            -O {output.vcf} \
            --tmp-dir={params.t} \
            -stand-call-conf 30 \
            -new-qual \
            -G StandardAnnotation \
            -G StandardHCAnnotation'
        
rule HC_concat_vcfs_gatk:
    '''
    Why both gatk and bcftools (below)?  Wen says Shalabh
    was just comparing.  Compare and make a decision?

    Note that the documentation for this tool says it needs
    the inputs to be in genomic order.  This should happen correctly,
    as the expand order will follow the chromList order.

    Changed from GatherVcfsCloud to GatherVcfs, as the former is
    still in beta.
    '''
    input:
        vcfList = expand(outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz', chrom=chromList)
    output:
        projectVCF = protected(outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz')
    params:
        e = gatkPath,
        vcfList_params = lambda wildcards, input:" -I ".join(input.vcfList)
    benchmark:
        'run_times/HC_concat_vcfs_gatk/build_final.tsv'
    shell:
        '{params.e}gatk --java-options "-Xmx4G" GatherVcfs -I {params.vcfList_params} -O {output.projectVCF}'
        
rule HC_concat_vcfs_bcftools:
    '''
    '''
    input:
        vcfList = expand(outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz', chrom=chromList),
        indexList = expand(outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz.tbi', chrom=chromList)
    output:
        projectVCF = protected(outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz')
    params:
        tempDir + 'HC_concat_vcfs_bcftools/'
    benchmark:
        'run_times/HC_concat_vcfs_bcftools/build_final.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load bcftools tabix;'
        'mkdir -p {params}; bcftools concat -a {input.vcfList} -Ou | bcftools sort -T {params} -Oz -o {output.projectVCF}'

rule HC_index_vcfs:
    '''
    '''
    input:
        vcf1 = outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz',
        vcf2 = outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz'
    output:
        idx1 = protected(outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz.tbi'),
        idx2 = protected(outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz.tbi')
    benchmark:
        'run_times/HC_index_vcfs/build_final.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input.vcf1};'
        'tabix -p vcf {input.vcf2}'


#######################################################################################################################
##################################################### DEEPVARIANT #####################################################
#######################################################################################################################

rule DV_make_examples:
    '''Generate TF examples for evaluation with DV models

    TODO: using shards test works; need to try with chroms.

    "consumes reads and the reference genome to create TensorFlow
    examples for evaluation with our deep learning models"

    --regions option: 
        - Can we run over each chromosome individually,
        to provide parallelization, rather than using shards?
        - Would we want to only call over the exome bed file?
        - Could we subset that by chrom in a rule and provide 
        the output to parallelize?

    --parallel, --task, N_SHARDS - all refer to sharded files,
    which may be obviated by splitting on chrom.  downside?

    safe to assume always gvcf, or should this be an option??


    This is how you do it with gnu parallel:
    'seq 0 {params.max} | parallel ____ make_examples --mode {params.mode} --ref {input.ref} --reads {input.bam} --regions {input.bed} --examples {output.ex} --gvcf {output.gvcf} --task \\x7B\\x7D'

        - seq: prints sequence of numbers from 1st param to 2nd param
        (e.g. 0 to 8-1 yields files numbering 0 to 7)

        - \\x7B\\x7D ASCII for { and }, to print literal curly braces
        (which will be replaced by the value of the seq number)

    For shards:
    - input for --examples MUST take the form of examples.tfrecord@10.gz,
    but output for snakemake should be in the format that the os will actually write
    which is examples.tfrecord-00000-of-00010
    - for the range of shards (n-of-N), n must start at 00000 and end at N-1 (this
    looks weird because it means you get a file 00009-of-00010 but NO 00010-of-00010)

    "DeepVariant can write sharded files using their filename@10-style name and can
    read sharded files using both that style as well as the glob form, such as
    filename-* or filename-*-of-00010"
    '''
    input:
        ref = refGenome,
        bam = inDir + '{sample}.bam',
        bai = inDir + '{sample}.bam.bai',
        bed = bedFile if useShards else outDir + 'split_regions/{chrom}.bed'
    output:
        ex = outDir + 'deepVariant/make_examples/{sample}.examples.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.examples.tfrecord.gz',
        gvcf = outDir + 'deepVariant/make_examples/{sample}.gvcf.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.gvcf.tfrecord.gz'
    params:
        mode = 'calling',
        ex = outDir + 'deepVariant/make_examples/{sample}.examples.tfrecord@' + str(numShards) + '.gz' if useShards else '',
        gvcf = outDir + 'deepVariant/make_examples/{sample}.gvcf.tfrecord@' + str(numShards) + '.gz' if useShards else ''
    benchmark:
        'run_times/DV_make_examples/{sample}_{shards}.tsv' if useShards else 'run_times/DV_make_examples/{sample}_{chrom}.tsv'
    run:
        if useShards:
            shell('source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;\
            make_examples \
                --mode {params.mode} \
                --ref {input.ref} \
                --reads {input.bam} \
                --regions {input.bed} \
                --examples {params.ex} \
                --gvcf {params.gvcf} \
                --task {wildcards.shards}')
        else:
            shell('source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;\
            make_examples \
                --mode {params.mode} \
                --ref {input.ref} \
                --reads {input.bam} \
                --regions {input.bed} \
                --examples {output.ex} \
                --gvcf {output.gvcf}')

rule DV_call_variants:
    '''Evaluate deep learning model to generate calls
    call_variants can accept sharded input but does not shard the output.
    At this step, the shards are re-combined.  If separating by chrom,
    the files remain split.

    Can't include model file as input, because it's only
    in the DV container.

    Intentionally NOT combining shards here to maintain parallelization.
    '''
    input:
        outDir + 'deepVariant/make_examples/{sample}.examples.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.examples.tfrecord.gz'
    output:
        outDir + 'deepVariant/called/tf_records/{sample}.variants.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/called/tf_records/{chrom}part_{sample}.variants.tfrecord.gz'
    params:
        path = modelPath,
        batch = '32'
    benchmark:
        'run_times/DV_call_variants/{sample}_{shards}.tsv' if useShards else 'run_times/DV_call_variants/{sample}_{chrom}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;'
        'call_variants \
            --outfile {output} \
            --examples {input} \
            --checkpoint {params.path} \
            --batch_size {params.batch}'

rule DV_postprocess_variants:
    '''Convert TFrecord calls to vcf and gvcf
    Intentionally NOT combining shards here to maintain parallelization.
    '''
    input:
        var = outDir + 'deepVariant/called/tf_records/{sample}.variants.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/called/tf_records/{chrom}part_{sample}.variants.tfrecord.gz',
        gvcf = outDir + 'deepVariant/make_examples/{sample}.gvcf.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.gvcf.tfrecord.gz',
        ref = refGenome
    output:
        vcf = outDir + 'deepVariant/called/vcfs/{sample}_record-{shards}-of-' + maxShards + '.vcf.gz' if useShards else outDir + 'deepVariant/called/vcfs/{chrom}part_{sample}_variants.vcf.gz',
        gvcf = outDir + 'deepVariant/called/gvcfs/{sample}_record-{shards}-of-' + maxShards + '.g.vcf.gz' if useShards else outDir + 'deepVariant/called/gvcfs/{chrom}part_{sample}_variants.g.vcf.gz'
    benchmark:
        'run_times/DV_postprocess_variants/{sample}_{shards}.tsv' if useShards else 'run_times/DV_postprocess_variants/{sample}_{chrom}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;'
        'postprocess_variants \
            --ref {input.ref} \
            --infile {input.var} \
            --outfile {output.vcf} \
            --nonvariant_site_tfrecord_path {input.gvcf} \
            --gvcf_outfile {output.gvcf}'

rule DV_index_vcfs:
    '''Index VCFs
    '''
    input:
        outDir + 'deepVariant/called/vcfs/{sample}_record-{shards}-of-' + maxShards + '.vcf.gz' if useShards else outDir + 'deepVariant/called/vcfs/{chrom}part_{sample}_variants.vcf.gz'
    output:
        outDir + 'deepVariant/called/vcfs/{sample}_record-{shards}-of-' + maxShards + '.vcf.gz.tbi' if useShards else outDir + 'deepVariant/called/vcfs/{chrom}part_{sample}_variants.vcf.gz.tbi'
    benchmark:
        'run_times/DV_index_vcfs/{sample}_{shards}.tsv' if useShards else 'run_times/DV_index_vcfs/{sample}_{chrom}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input}'

rule DV_concat_vcfs:
    '''Concatenate vcfs
    You can just cat shards to merge - 
    https://github.com/google/deepvariant/issues/113
    '''
    input:
        vcf = expand(outDir + 'deepVariant/called/vcfs/{{sample}}_record-{shards}-of-' + maxShards + '.vcf.gz', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/vcfs/{chrom}part_{{sample}}_variants.vcf.gz', chrom=chromList),
        idx = expand(outDir + 'deepVariant/called/vcfs/{{sample}}_record-{shards}-of-' + maxShards + '.vcf.gz.tbi', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/vcfs/{chrom}part_{{sample}}_variants.vcf.gz.tbi', chrom=chromList)
    output:
        gz = outDir + 'deepVariant/called/vcfs/{sample}_all_chroms.vcf.gz'
    params:
        tempDir + 'DV_concat_vcfs/{sample}/'
    benchmark:
        'run_times/DV_concat_vcfs/{sample}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load bcftools;'
        'mkdir -p {params}; bcftools concat -Ou -a {input.vcf} | bcftools sort -T {params} -Oz -o {output.gz}' # sort -k1,1 -k2,2n > {output}'

rule DV_index_gvcfs:
    '''Index gVCFs
    '''
    input:
        outDir + 'deepVariant/called/gvcfs/{sample}_record-{shards}-of-' + maxShards + '.g.vcf.gz' if useShards else outDir + 'deepVariant/called/gvcfs/{chrom}part_{sample}_variants.g.vcf.gz'
    output:
        outDir + 'deepVariant/called/gvcfs/{sample}_record-{shards}-of-' + maxShards + '.g.vcf.gz.tbi' if useShards else outDir + 'deepVariant/called/gvcfs/{chrom}part_{sample}_variants.g.vcf.gz.tbi'
    benchmark:
        'run_times/DV_index_gvcfs/{sample}_{shards}.tsv' if useShards else 'run_times/DV_index_gvcfs/{sample}_{chrom}.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input}'

rule DV_concat_gvcfs:
    '''Concatenate gvcfs
    You can just cat shards to merge - 
    https://github.com/google/deepvariant/issues/113

    GLnexus works on uncompressed gvcfs and doesn't use index
    '''
    input:
        vcf = expand(outDir + 'deepVariant/called/gvcfs/{{sample}}_record-{shards}-of-' + maxShards + '.g.vcf.gz', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/gvcfs/{chrom}part_{{sample}}_variants.g.vcf.gz', chrom=chromList),
        idx = expand(outDir + 'deepVariant/called/gvcfs/{{sample}}_record-{shards}-of-' + maxShards + '.g.vcf.gz.tbi', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/gvcfs/{chrom}part_{{sample}}_variants.g.vcf.gz.tbi', chrom=chromList)
    output:
        vcf = outDir + 'deepVariant/called/gvcfs/{sample}_all_chroms.g.vcf'
    benchmark:
        'run_times/DV_concat_gvcfs/{sample}.tsv'
    params:
        tempDir + 'DV_concat_gvcfs/{sample}/'
    shell:
        'source /etc/profile.d/modules.sh; module load bcftools;'
        'mkdir -p {params}; bcftools concat -Ou -a {input.vcf} | bcftools sort -T {params} -Ov -o {output.vcf}' # sort -k1,1 -k2,2n > {output}'

rule DV_create_manifest:
    '''Create list of files for GLnexus to merge
    '''
    input:
        expand(outDir + 'deepVariant/called/gvcfs/{sample}_all_chroms.g.vcf', sample=sampleList)
    output:
        'deepVariant/called/gvcfs/manifest.txt'
    benchmark:
        'run_times/DV_create_manifest/manifest.tsv'
    shell:
        'echo {input} | tr " " "\n" > {output}'

rule DV_GLmerge_gvcfs:
    '''Merge gvcfs into one multi-sample vcf

    "The glnexus_cli executable consumes the gVCF files, and a three-column 
    BED file giving the genomic ranges to analyze. For exomes, the BED file 
    might contain the exome capture targets with some padding, while for 
    WGS you can just give the full-length chromosomes."

    Are shards are equivalent regions across samples?  Probably not?
    Then can't parallelize by shard here.  Do we need parallelization?
    If so, should we first split by region?

    GLnexus outputs uncompressed bcf.

    GLnexus creates a directory called "GLnexus.DB" in the working directory.  Track?
    If this step fails, then the database GLnexus.DB already exists, and resuming
    the pipeline will result in an error.  Add a timestamp?

    '''
    input:
        l = expand(outDir + 'deepVariant/called/gvcfs/{sample}_all_chroms.g.vcf', sample=sampleList),
        m = 'deepVariant/called/gvcfs/manifest.txt',
        b = bedFile
    output: 
        temp(outDir + 'deepVariant/genotyped/build_final.bcf')
    benchmark:
        'run_times/DV_GLmerge_gvcfs/build_final.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load glnexus/1.1.5 gcc;'
        'glnexus_cli --config DeepVariant --bed {input.b} --list {input.m} > {output}'

rule DV_compress_merged_vcfs:
    '''
    '''
    input:
        outDir + 'deepVariant/genotyped/build_final.bcf'
    output:
        gz = protected(outDir + 'deepVariant/genotyped/build_final.vcf.gz'),
        tbi = protected(outDir + 'deepVariant/genotyped/build_final.vcf.gz.tbi')
    benchmark:
        'run_times/DV_compress_merged_vcfs/build_final.tsv'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix bcftools;'
        'bcftools view {input} | bgzip -c > {output.gz}; tabix -p vcf {output.gz}'
