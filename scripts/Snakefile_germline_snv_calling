#!/usr/bin/env python3
# Authors: Shalabh Suman, Bari Ballew

####################run on 72 CTRL WES samples and on 2 syndip WGS samples

'''Joint variant calling with GATK HaplotypeCaller and Google DeepVariant.

Notes:
    This pipeline is for use with the production germline pipeline.
    Therefore, some things are assumed, e.g. that input bams are indexed,
    that the reference genome is appropriately indexed, etc.  The pipeline
    will halt if these assumptions are not true, but there are no rules
    to perform these tasks.

Input:
    Customized config.yaml, sorted/indexed bams

Output:
    Merged multi-sample VCFs, one called with HaplotypeCaller, and one
    called with DeepVariant

'''


import os
import glob

# reference the config file
conf = os.environ.get("conf")
configfile: conf

# import variables from the config file
refGenome = config['refGenome']
gatkPath = config['gatkPath']
intervalFile = config['intervalFile']
bedFile = config['bedFile']
outDir = config['outputDir']
inDir = config['inputDir']
tempDir=config['tempDir']
logDir = config['logDir']
modelPath = config['modelPath']
useShards = config['useShards']
if useShards:
    numShards = config['numShards']
    maxShards = "{:05d}".format(numShards)
    s = list(range(numShards))
    shardsList = ["%05d" % n for n in s]

# derive additional refgenome variables
refFile = os.path.basename(refGenome)
refDir = os.path.dirname(refGenome)
refNoExt = os.path.splitext(refFile)[0]
dictionaryFile = refDir + '/' + refNoExt + '.dict'

# set a datetime stamp for GATK DBImport step
dt = str(config['dt'])

# read all *.bam filenames in input directory into list (assume bams are indexed)
bamList = [f for f in os.listdir(inDir) if f.endswith('.bam')]

# generate sampleList based on bam filenames
sampleList = [os.path.splitext(f)[0] for f in bamList]

# read in chromosome list from reference dict file (assumes the dict is already created)
chromList = []
with open(dictionaryFile) as f:
    next(f)
    for line in f:
        f1 = line.split("\t")[1]
        f2 = f1.split(":")[1]
        chromList.append(f2)

def get_DBImport_path1(wildcards):
    return(glob.glob(outDir + 'HaplotypeCaller/DBImport/' + dt + '_' + wildcards.chrom + '/' + wildcards.chrom + '*/genomicsdb_meta_dir/genomicsdb_meta*.json'))

def get_DBImport_path2(wildcards):
    path = ''.join(glob.glob(outDir + 'HaplotypeCaller/DBImport/' + dt + '_' + wildcards.chrom + '/*/__*/'))
    myList = []
    if os.path.exists(path):
        myList = ['AD.tdb', 'AD_var.tdb', 'ALT.tdb', 'ALT_var.tdb', 'BaseQRankSum.tdb', '__book_keeping.tdb.gz', '__coords.tdb', 'DP_FORMAT.tdb', 'DP.tdb', 'DS.tdb', 'END.tdb', 'ExcessHet.tdb', 'FILTER.tdb', 'FILTER_var.tdb', 'GQ.tdb', 'GT.tdb', 'GT_var.tdb', 'ID.tdb', 'ID_var.tdb', 'InbreedingCoeff.tdb', 'MIN_DP.tdb', 'MLEAC.tdb', 'MLEAC_var.tdb', 'MLEAF.tdb', 'MLEAF_var.tdb', 'MQRankSum.tdb', 'PGT.tdb', 'PGT_var.tdb', 'PID.tdb', 'PID_var.tdb', 'PL.tdb', 'PL_var.tdb', 'QUAL.tdb', 'RAW_MQandDP.tdb', 'ReadPosRankSum.tdb', 'REF.tdb', 'REF_var.tdb', 'SB.tdb', '__tiledb_fragment.tdb']
        myList = [path + file for file in myList]
    return(myList)

report: 'report/workflow.rst'


rule all:
    input:
        outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz.tbi',
        outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz.tbi',
        expand(outDir + 'deepVariant/called/vcfs/{sample}_variants.vcf.gz', sample=sampleList),
        outDir + 'deepVariant/genotyped/build_final.vcf.gz.tbi'

#######################################################################################################################
################################################ GATK HAPLOTYPE CALLER ################################################
#######################################################################################################################

rule HC_call_variants:
    '''Call gVCFs with GATK4
    '''
    input:
        ref = refGenome,
        i1 = refGenome + '.amb',
        i2 = refGenome + '.ann',
        i3 = refGenome + '.bwt',
        i4 = refGenome + '.pac',
        i5 = refGenome + '.sa',
        i6 = refGenome + '.fai',
        i7 = dictionaryFile,
        interval =  intervalFile,
        bam = inDir + '{sample}.bam',
        bai = inDir + '{sample}.bam.bai'
    output:
        gvcf = outDir + 'HaplotypeCaller/called/{sample}.g.vcf'
    params:
        e = gatkPath
    threads: 5
    # singularity: 'docker://broadinstitute/gatk'
    shell:
        '{params.e}gatk --java-options "-Xmx4G" HaplotypeCaller \
            -R {input.ref} \
            -I {input.bam} \
            -ERC GVCF \
            -L {input.interval} \
            -O {output.gvcf} \
            -new-qual \
            -G StandardAnnotation \
            -G StandardHCAnnotation'

rule HC_compress_gvcfs:
    '''Zip and index gVCFs
    '''
    input:
        outDir + 'HaplotypeCaller/called/{sample}.g.vcf'
    output:
        outDir + 'HaplotypeCaller/called/{sample}.g.vcf.gz',
        outDir + 'HaplotypeCaller/called/{sample}.g.vcf.gz.tbi'
    threads: 2
    # singularity: '...'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix bgzip;'
        'bgzip {input}; tabix -p vcf {input}.gz'

rule HC_genomicsDBImport:
    '''Split samples by chromosome

    The output of this step includes some files that are in subdirectories
    with unpredictable names.  I've attempted to include them in the input
    of the next rule via glob in the functions below (o5 and o6 in rule
    HC_genotypeGVCFs) but this is still being tested.

    Note that DBImport requires a new or empty directory for 
    --genomicsdb-workspace-path.  Possible issue when resuming a pipeline.
    Snakemake's implicit directory management results in an error when
    DBImport finds that the workspace path already exists (even though it
    seems empty?).  Removing the directory just prior to running DBImport
    seems to solve this problem, but will be problematic on resuming the 
    pipeline.  Added a datetime stamp (dt) to the dir to help address this.
    However, this datetime stamp will need to be overriden if the pipeline
    is stopped and then resumed prior to completion of rule HC_genotypeGVCFs.
    '''
    input: 
        gvcfList = expand(outDir + "HaplotypeCaller/called/{sample}.g.vcf.gz", sample=sampleList)
    output:
        o1 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vcfheader.vcf',
        o2 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vidmap.json',
        o3 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/callset.json',
        o4 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/__tiledb_workspace.tdb'
    params:
        e = gatkPath,
        gvcfList = lambda wildcards, input:" -V ".join(input.gvcfList),
        interval = '{chrom}',
        db = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}'
    threads: 10
    shell:
        'rm -r {params.db}; {params.e}gatk --java-options "-Xmx4G" GenomicsDBImport -V {params.gvcfList} --genomicsdb-workspace-path {params.db} -L {params.interval}'

rule HC_genotype_gvcfs:
    '''Joint genotyping
    '''
    input:
        ref = refGenome,
        interval =  intervalFile,
        o1 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vcfheader.vcf',
        o2 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/vidmap.json',
        o3 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/callset.json',
        o4 = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}/__tiledb_workspace.tdb',
        o5 = get_DBImport_path1,
        o6 = get_DBImport_path2
    output:
        vcf = outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz',
        idx = outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz.tbi'
    params:
        e = gatkPath,
        db = outDir + 'HaplotypeCaller/DBImport/' + dt + '_{chrom}',
        temp = tempDir
    threads: 5
    shell:
        '{params.e}gatk --java-options "-Xmx4G" GenotypeGVCFs \
            -R {input.ref} \
            -V gendb://{params.db} \
            -O {output.vcf} \
            --tmp-dir={params.temp} \
            -new-qual -stand-call-conf 30 \
            -G StandardAnnotation \
            -G StandardHCAnnotation'
        
rule HC_concat_vcfs_gatk:
    '''
    Why both gatk and bcftools (below)?  Wen says Shalabh
    was just comparing.  Compare and make a decision?
    '''
    input:
        vcfList = expand(outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz', chrom=chromList)
    output:
        projectVCF = protected(outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz')
    params:
        e = gatkPath,
        vcfList_params = lambda wildcards, input:" -I ".join(input.vcfList)
    threads: 10
    shell:
        '{params.e}gatk --java-options "-Xmx4G" GatherVcfsCloud -I {params.vcfList_params} -O {output.projectVCF}'
        
rule HC_concat_vcfs_bcftools:
    '''
    '''
    input:
        vcfList = expand(outDir + 'HaplotypeCaller/genotyped/{chrom}.vcf.gz', chrom=chromList)
    output:
        projectVCF = protected(outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz')
    threads: 5
    shell:
        'source /etc/profile.d/modules.sh; module load bcftools tabix;'
        'bcftools concat {input.vcfList} -Oz -o {output.projectVCF}'

rule HC_index_vcfs:
    '''
    '''
    input:
        vcf1 = outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz',
        vcf2 = outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz'
    output:
        idx1 = protected(outDir + 'HaplotypeCaller/genotyped/combined/gatk/build_final.vcf.gz.tbi'),
        idx2 = protected(outDir + 'HaplotypeCaller/genotyped/combined/bcftools/build_final.vcf.gz.tbi')
    threads: 5
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input.vcf1};'
        'tabix -p vcf {input.vcf2}'


#######################################################################################################################
##################################################### DEEPVARIANT #####################################################
#######################################################################################################################

if not useShards:
    rule split_bed_file:
        '''Separates bed regions by chromosome.

        If you're not assigning a number of shards by which to divide
        and parallelize, then the pipeline will parallelize by chrom.
        To do this, we take the bed file (e.g. exome capture region)
        and split the regions by chromosome.  Subsequent steps are run
        concurrently on each of the single-chromosome bed files.

        Note that grep exits with 0 if a match is found, 1 if no match,
        and 2 if error.  Snakemake looks for exit codes of 0 to determine
        that a job finished successfully.  No match is an acceptable outcome
        here, so the shell command below should allow match or no match.

        don't love the ||true solution; what will it do for exit > 1?
        '''
        input:
            intervalFile
        output:
            outDir + 'split_beds/{chrom}.bed'
        shell:
            'grep {wildcards.chrom} {input} > {output} || true'
            #'grep -m1 {wildcards.chrom} {input}; if [ $? -lt 2 ]; then grep {wildcards.chrom} {input} > {output}; fi'


rule DV_make_examples:
    '''Generate TF examples for evaluation with DV models

    TODO: using shards test works; need to try with chroms.

    "consumes reads and the reference genome to create TensorFlow
    examples for evaluation with our deep learning models"

    --regions option: 
        - Can we run over each chromosome individually,
        to provide parallelization, rather than using shards?
        - Would we want to only call over the exome bed file?
        - Could we subset that by chrom in a rule and provide 
        the output to parallelize?

    --parallel, --task, N_SHARDS - all refer to sharded files,
    which may be obviated by splitting on chrom.  downside?

    safe to assume always gvcf, or should this be an option??


    This is how you do it with gnu parallel:
    'seq 0 {params.max} | parallel ____ make_examples --mode {params.mode} --ref {input.ref} --reads {input.bam} --regions {input.bed} --examples {output.ex} --gvcf {output.gvcf} --task \\x7B\\x7D'

        - seq: prints sequence of numbers from 1st param to 2nd param
        (e.g. 0 to 8-1 yields files numbering 0 to 7)

        - \\x7B\\x7D ASCII for { and }, to print literal curly braces
        (which will be replaced by the value of the seq number)

    For shards:
    - input for --examples MUST take the form of examples.tfrecord@10.gz,
    but output for snakemake should be in the format that the os will actually write
    which is examples.tfrecord-00000-of-00010
    - for the range of shards (n-of-N), n must start at 00000 and end at N-1 (this
    looks weird because it means you get a file 00009-of-00010 but NO 00010-of-00010)

    "DeepVariant can write sharded files using their filename@10-style name and can
    read sharded files using both that style as well as the glob form, such as
    filename-* or filename-*-of-00010"
    '''
    input:
        ref = refGenome,
        bam = inDir + '{sample}.bam',
        bai = inDir + '{sample}.bam.bai',
        bed = bedFile if useShards else outDir + 'split_beds/{chrom}.bed'
    output:
        ex = outDir + 'deepVariant/make_examples/{sample}.examples.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.examples.tfrecord.gz',
        gvcf = outDir + 'deepVariant/make_examples/{sample}.gvcf.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.gvcf.tfrecord.gz'
    params:
        mode = 'calling',
        ex = outDir + 'deepVariant/make_examples/{sample}.examples.tfrecord@' + str(numShards) + '.gz' if useShards else '',
        gvcf = outDir + 'deepVariant/make_examples/{sample}.gvcf.tfrecord@' + str(numShards) + '.gz' if useShards else ''
    run:
        if useShards:
            shell('source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;\
            make_examples \
                --mode {params.mode} \
                --ref {input.ref} \
                --reads {input.bam} \
                --regions {input.bed} \
                --examples {params.ex} \
                --gvcf {params.gvcf} \
                --task {wildcards.shards}')
        else:
            shell('source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;\
            make_examples \
                --mode {params.mode} \
                --ref {input.ref} \
                --reads {input.bam} \
                --regions {input.bed} \
                --examples {output.ex} \
                --gvcf {output.gvcf}')

rule DV_call_variants:
    '''Evaluate deep learning model to generate calls
    call_variants can accept sharded input but does not shard the output.
    At this step, the shards are re-combined.  If separating by chrom,
    the files remain split.

    Can't include model file as input, because it's only
    in the DV container.

    Intentionally NOT combining shards here to maintain parallelization.
    '''
    input:
        outDir + 'deepVariant/make_examples/{sample}.examples.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.examples.tfrecord.gz'
    output:
        outDir + 'deepVariant/called/tf_records/{sample}.variants.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/called/tf_records/{chrom}part_{sample}.variants.tfrecord.gz'
    params:
        path = modelPath,
        batch = '32'
    shell:
        'source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;'
        'call_variants \
            --outfile {output} \
            --examples {input} \
            --checkpoint {params.path} \
            --batch_size {params.batch}'

rule DV_postprocess_variants:
    '''Convert TFrecord calls to vcf and gvcf
    Intentionally NOT combining shards here to maintain parallelization.
    '''
    input:
        var = outDir + 'deepVariant/called/tf_records/{sample}.variants.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/called/tf_records/{chrom}part_{sample}.variants.tfrecord.gz',
        gvcf = outDir + 'deepVariant/make_examples/{sample}.gvcf.tfrecord-{shards}-of-' + maxShards + '.gz' if useShards else outDir + 'deepVariant/make_examples/{chrom}part_{sample}.gvcf.tfrecord.gz',
        ref = refGenome
    output:
        vcf = outDir + 'deepVariant/called/vcfs/{sample}_record-{shards}-of-' + maxShards + '.vcf.gz' if useShards else outDir + 'deepVariant/called/vcfs/{chrom}part_{sample}_variants.vcf.gz',
        gvcf = outDir + 'deepVariant/called/gvcfs/{sample}_record-{shards}-of-' + maxShards + '.g.vcf.gz' if useShards else outDir + 'deepVariant/called/gvcfs/{chrom}part_{sample}_variants.g.vcf.gz'
    shell:
        'source /etc/profile.d/modules.sh; module load deepvariant/0.5.2;'
        'postprocess_variants \
            --ref {input.ref} \
            --infile {input.var} \
            --outfile {output.vcf} \
            --nonvariant_site_tfrecord_path {input.gvcf} \
            --gvcf_outfile {output.gvcf}'

rule DV_index_vcfs:
    '''Index VCFs
    '''
    input:
        outDir + 'deepVariant/called/vcfs/{sample}_record-{shards}-of-' + maxShards + '.vcf.gz' if useShards else outDir + 'deepVariant/called/vcfs/{chrom}part_{sample}_variants.vcf.gz'
    output:
        outDir + 'deepVariant/called/vcfs/{sample}_record-{shards}-of-' + maxShards + '.vcf.gz.tbi' if useShards else outDir + 'deepVariant/called/vcfs/{chrom}part_{sample}_variants.vcf.gz.tbi'
    threads: 2
    # singularity: '...'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input}'

rule DV_concat_vcfs:
    '''Concatenate vcfs
    You can just cat shards to merge - 
    https://github.com/google/deepvariant/issues/113
    '''
    input:
        vcf = expand(outDir + 'deepVariant/called/vcfs/{{sample}}_record-{shards}-of-' + maxShards + '.vcf.gz', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/vcfs/{chrom}part_{{sample}}_variants.vcf.gz', chrom=chromList),
        idx = expand(outDir + 'deepVariant/called/vcfs/{{sample}}_record-{shards}-of-' + maxShards + '.vcf.gz.tbi', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/vcfs/{chrom}part_{{sample}}_variants.vcf.gz.tbi', chrom=chromList)
    output:
        outDir + 'deepVariant/called/vcfs/{sample}_variants.vcf.gz'
    shell:
        'source /etc/profile.d/modules.sh; module load bcftools;'
        'bcftools concat -Ou -a {input.vcf} | bcftools sort -Oz -o {output}' # sort -k1,1 -k2,2n > {output}'

rule DV_index_gvcfs:
    '''Index gVCFs
    '''
    input:
        outDir + 'deepVariant/called/gvcfs/{sample}_record-{shards}-of-' + maxShards + '.g.vcf.gz' if useShards else outDir + 'deepVariant/called/gvcfs/{chrom}part_{sample}_variants.g.vcf.gz'
    output:
        outDir + 'deepVariant/called/gvcfs/{sample}_record-{shards}-of-' + maxShards + '.g.vcf.gz.tbi' if useShards else outDir + 'deepVariant/called/gvcfs/{chrom}part_{sample}_variants.g.vcf.gz.tbi'
    threads: 2
    # singularity: '...'
    shell:
        'source /etc/profile.d/modules.sh; module load tabix;'
        'tabix -p vcf {input}'

rule DV_concat_gvcfs:
    '''Concatenate gvcfs
    You can just cat shards to merge - 
    https://github.com/google/deepvariant/issues/113

    GLnexus works on uncompressed gvcfs and doesn't use index
    '''
    input:
        vcf = expand(outDir + 'deepVariant/called/gvcfs/{{sample}}_record-{shards}-of-' + maxShards + '.g.vcf.gz', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/gvcfs/{chrom}part_{{sample}}_variants.g.vcf.gz', chrom=chromList),
        idx = expand(outDir + 'deepVariant/called/gvcfs/{{sample}}_record-{shards}-of-' + maxShards + '.g.vcf.gz.tbi', shards=shardsList) if useShards else expand(outDir + 'deepVariant/called/gvcfs/{chrom}part_{{sample}}_variants.g.vcf.gz.tbi', chrom=chromList)
    output:
        outDir + 'deepVariant/called/gvcfs/{sample}_variants.g.vcf'
    shell:
        'source /etc/profile.d/modules.sh; module load bcftools;'
        'bcftools concat -Ou -a {input.vcf} | bcftools sort -Ov -o {output}' # sort -k1,1 -k2,2n > {output}'

rule DV_create_manifest:
    '''Create list of files for GLnexus to merge
    '''
    input:
        expand(outDir + 'deepVariant/called/gvcfs/{sample}_variants.g.vcf', sample=sampleList)
    output:
        'deepVariant/called/gvcfs/manifest.txt'
    shell:
        'echo {input} | tr " " "\n" > {output}'

rule DV_GLmerge_gvcfs:
    '''Merge gvcfs into one multi-sample vcf

    The glnexus_cli executable consumes the gVCF files, and a three-column BED file giving the genomic ranges to analyze. For exomes, the BED file might contain the exome capture targets with some padding, while for WGS you can just give the full-length chromosomes.

    Are shards are equivalent regions across samples?  Probably not?
    Then can't parallelize by shard here.  Do we need parallelization?
    If so, should we first split by region?

    GLnexus outputs uncompressed bcf.

    GLnexus creates a directory called "GLnexus.DB" in the working directory.  Track?

    '''
    input:
        l = expand(outDir + 'deepVariant/called/gvcfs/{sample}_variants.g.vcf', sample=sampleList),
        m = 'deepVariant/called/gvcfs/manifest.txt',
        b = bedFile
    output: 
        temp(outDir + 'deepVariant/genotyped/build_final.bcf')
    shell:
        'source /etc/profile.d/modules.sh; module load glnexus/1.1.5 gcc;'
        'glnexus_cli --config DeepVariant --bed {input.b} --list {input.m} > {output}'

rule DV_compress_merged_vcfs:
    '''
    '''
    input:
        outDir + 'deepVariant/genotyped/build_final.bcf'
    output:
        gz = protected(outDir + 'deepVariant/genotyped/build_final.vcf.gz'),
        tbi = protected(outDir + 'deepVariant/genotyped/build_final.vcf.gz.tbi')
    shell:
        'source /etc/profile.d/modules.sh; module load tabix bcftools;'
        'bcftools view {input} | bgzip -c > {output.gz}; tabix -p vcf {output.gz}'
